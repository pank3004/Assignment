{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared (R²) represents the proportion of variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, where:\n",
    "\n",
    "# 0 indicates that the model explains none of the variability.\n",
    "# 1 indicates that the model explains all of the variability.\n",
    "# It is calculated as the ratio of the explained variance to the total variance. A higher R² value indicates a better fit, meaning the model explains more of the data’s variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted R-squared adjusts R² for the number of predictors in the model. Unlike regular R-squared, which can increase as more variables are added (even if they don't improve the model), adjusted R² penalizes the model for adding irrelevant predictors. It provides a more accurate measure of the model’s performance by accounting for the complexity of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It’s more appropriate to use adjusted R-squared when comparing models with different numbers of predictors. Since it accounts for model complexity, it helps prevent overfitting by not artificially inflating the R² when irrelevant variables are added.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE (Root Mean Squared Error): The square root of the average squared differences between predicted and actual values. It gives more weight to larger errors due to squaring.\n",
    "# MSE (Mean Squared Error): The average squared difference between predicted and actual values. Like RMSE, it emphasizes larger errors.\n",
    "# MAE (Mean Absolute Error): The average of absolute differences between predicted and actual values. It treats all errors equally, without squaring them.\n",
    "# They all measure model performance by indicating how far predictions are from actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE:\n",
    "\n",
    "# Advantages: Penalizes larger errors, making it more sensitive to outliers.\n",
    "# Disadvantages: Can be difficult to interpret due to squaring the error, and may overemphasize large errors.\n",
    "# MSE:\n",
    "\n",
    "# Advantages: Commonly used, and easily interpretable in terms of squared units.\n",
    "# Disadvantages: Also sensitive to outliers and not in the same unit as the original data.\n",
    "# MAE:\n",
    "\n",
    "# Advantages: More interpretable since it’s in the same unit as the data, less sensitive to outliers than RMSE and MSE.\n",
    "# Disadvantages: Does not penalize large errors as strongly, so it may underemphasize large errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso regularization (Least Absolute Shrinkage and Selection Operator) adds a penalty to the regression model based on the absolute value of coefficients. Lasso can shrink some coefficients to zero, which leads to feature selection.\n",
    "\n",
    "# Ridge regularization penalizes based on the square of the coefficients but does not shrink any coefficients to zero, so all features remain in the model.\n",
    "\n",
    "# Lasso is better for models where some features are irrelevant and can be discarded.\n",
    "# Ridge is better when all features are relevant but need to be regularized to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized models, such as Lasso and Ridge, penalize large coefficients, forcing the model to be simpler and less likely to overfit the training data. This is particularly useful when dealing with noisy data or when the number of features is high relative to the number of observations.\n",
    "\n",
    "# Example: In a house price prediction model, using Lasso might shrink the coefficients of irrelevant features like \"year the house was painted\" to zero, effectively removing them from the model and reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Interpretability: Regularization changes the model’s coefficients, making it harder to interpret the importance of features.\n",
    "# Sensitive to Hyperparameters: Regularization relies on choosing the correct penalty parameter (lambda), which can be tricky.\n",
    "# Non-linear Relationships: Regularized linear models are still linear models, so they may fail to capture complex, non-linear relationships.\n",
    "# In cases where non-linearity is important, models like decision trees, random forests, or neural networks may perform better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Model A has an RMSE of 10 and Model B has an MAE of 8, choosing between them depends on your priorities:\n",
    "\n",
    "# Model A: RMSE gives more weight to larger errors. If large errors are particularly bad in your use case, Model A might perform better since its focus is on reducing big errors.\n",
    "# Model B: MAE treats all errors equally. If consistency in errors is more important than avoiding large errors, Model B would be the better choice.\n",
    "# Limitations: RMSE can overemphasize outliers, while MAE might underemphasize larger errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A uses Ridge regularization (parameter = 0.1), and Model B uses Lasso regularization (parameter = 0.5):\n",
    "\n",
    "# Model A (Ridge): Best if all predictors are useful but need to be shrunk to prevent overfitting.\n",
    "# Model B (Lasso): Better if you suspect some predictors are irrelevant, as Lasso can remove them by shrinking their coefficients to zero.\n",
    "# Trade-offs:\n",
    "\n",
    "# Lasso might be more beneficial for sparse models where irrelevant variables need to be removed.\n",
    "# Ridge is preferable when all variables contribute to the outcome and should be retained in the model.\n",
    "# Limitations: Both models require tuning of the regularization parameter, and over-regularization can cause underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
