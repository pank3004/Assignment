{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that adds L1 regularization to the cost function. The L1 regularization penalizes the absolute values of the coefficients, encouraging the model to reduce the magnitude of some coefficients to zero. This leads to simpler models by effectively performing feature selection, where less important features are eliminated.\n",
    "\n",
    "# Difference from Other Regression Techniques:\n",
    "\n",
    "# Ordinary Least Squares (OLS): Minimizes the sum of squared residuals without any penalty, so it includes all features.\n",
    "# Ridge Regression: Adds L2 regularization (penalizes the squared values of the coefficients), which shrinks coefficients but doesn’t eliminate them.\n",
    "# Elastic Net Regression: Combines both L1 (like Lasso) and L2 (like Ridge) penalties, allowing for both feature selection and shrinkage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main advantage of Lasso Regression in feature selection is its ability to shrink the coefficients of less important features to exactly zero, effectively eliminating irrelevant or redundant features. This makes Lasso particularly useful for high-dimensional data with many predictors, as it reduces model complexity and improves interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Lasso Regression, the coefficients are interpreted similarly to those in linear regression:\n",
    "\n",
    "# A coefficient represents the change in the target variable for a one-unit change in the corresponding feature, holding all other variables constant.\n",
    "# However, some coefficients will be exactly zero due to the L1 penalty, indicating that those features do not contribute to the model.\n",
    "# Non-zero coefficients can be interpreted normally, though their values will be smaller compared to an OLS model due to the regularization effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main tuning parameter in Lasso Regression is the regularization parameter (lambda or α). It controls the strength of the L1 penalty.\n",
    "\n",
    "# High lambda (large penalty): Forces more coefficients to zero, increasing feature elimination and simplifying the model but potentially increasing bias (underfitting).\n",
    "# Low lambda (small penalty): Reduces the regularization effect, leading to a model closer to OLS with more non-zero coefficients, which can reduce bias but increase variance (overfitting).\n",
    "# Tuning this parameter is crucial for balancing bias and variance in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, Lasso Regression can be used for non-linear regression problems by incorporating feature engineering. This is typically done by transforming the original features into higher-dimensional spaces (e.g., using polynomial features, splines, or basis functions) and then applying Lasso Regression. Lasso itself remains linear in the parameters, but the non-linear transformations of the features allow it to capture non-linear relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The key difference between Ridge and Lasso Regression lies in the type of regularization used:\n",
    "\n",
    "# Lasso Regression: Uses L1 regularization, which tends to eliminate features by driving some coefficients to zero, effectively performing feature selection.\n",
    "# Ridge Regression: Uses L2 regularization, which shrinks the coefficients but does not eliminate any features. All features are retained, even if their influence is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, Lasso Regression can handle multicollinearity by selecting one of the correlated features and shrinking the others to zero. When two or more features are highly correlated, Lasso will often keep just one of the correlated features with a non-zero coefficient and eliminate the others, thus reducing multicollinearity in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- The optimal value of the regularization parameter (lambda) is typically chosen using cross-validation. This process involves:\n",
    "\n",
    "Dividing the data into training and validation sets.\n",
    "Training the Lasso model on different values of lambda (using a range of values).\n",
    "Evaluating the model's performance on the validation set for each value of lambda.\n",
    "Selecting the lambda that results in the best performance (usually minimizing the validation error or maximizing the validation score).\n",
    "This ensures that the model generalizes well to unseen data. -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
