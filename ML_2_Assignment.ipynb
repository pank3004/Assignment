{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting occurs when a model learns the training data too well, capturing noise and outliers, which leads to poor generalization to new, unseen data. Consequences include high accuracy on training data but low accuracy on validation/test data.\n",
    "\n",
    "# Mitigation strategies:\n",
    "\n",
    "# Use simpler models.\n",
    "# Reduce the number of features (feature selection).\n",
    "# Apply regularization techniques (like L1 or L2).\n",
    "# Use cross-validation to tune model parameters.\n",
    "# Prune decision trees.\n",
    "# Underfitting happens when a model is too simple to capture the underlying patterns in the data. Consequences include poor performance on both training and test data.\n",
    "\n",
    "# Mitigation strategies:\n",
    "\n",
    "# Increase model complexity (e.g., using more complex algorithms).\n",
    "# Improve feature selection or engineering.\n",
    "# Reduce regularization strength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify the model: Use a less complex model to avoid capturing noise.\n",
    "# Regularization: Apply techniques like L1 (Lasso) or L2 (Ridge) to penalize large coefficients.\n",
    "# Cross-validation: Validate the model using techniques like k-fold cross-validation to ensure it generalizes well.\n",
    "# Early stopping: Monitor performance on validation data during training and stop when performance begins to decline.\n",
    "# Increase training data: More data can help the model learn better representations and reduce the likelihood of fitting noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Underfitting is when a model is too simple to capture the complexity of the data. It results in poor performance on both training and test datasets.\n",
    "\n",
    "# Scenarios where underfitting can occur:\n",
    "\n",
    "# Using a linear model for a non-linear problem.\n",
    "# Insufficient training time, leading to a model that hasnâ€™t converged.\n",
    "# Too much regularization, which can overly constrain the model.\n",
    "# Using very few features when the true relationship requires more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bias-variance tradeoff refers to the balance between two types of errors that affect model performance:\n",
    "\n",
    "# Bias: Error due to overly simplistic assumptions in the learning algorithm. High bias leads to underfitting (the model cannot capture the underlying trends).\n",
    "\n",
    "# Variance: Error due to excessive sensitivity to fluctuations in the training data. High variance leads to overfitting (the model captures noise rather than the signal).\n",
    "\n",
    "# Relationship: Increasing model complexity typically decreases bias but increases variance, while simplifying the model reduces variance but increases bias. The goal is to find an optimal point that minimizes total error, balancing both bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves: Plot training and validation errors. If training error is low and validation error is high, it indicates overfitting. If both are high, it indicates underfitting.\n",
    "# Cross-validation: Evaluate model performance on different subsets of data. Significant differences between training and validation scores can indicate overfitting.\n",
    "# Model performance metrics: Check metrics like accuracy, precision, recall, F1 score on training vs. validation/test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High Bias:\n",
    "\n",
    "# Examples: Linear regression on non-linear data, overly simple models.\n",
    "# Performance: Poor performance on both training and test data; consistently misses relevant relations.\n",
    "# High Variance:\n",
    "\n",
    "# Examples: Decision trees without pruning, k-nearest neighbors with low k.\n",
    "# Performance: Good performance on training data but poor on test data; captures noise as if it were a true pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization is a technique used to prevent overfitting by adding a penalty to the loss function based on the complexity of the model. It discourages overly complex models, helping to improve generalization.\n",
    "\n",
    "# Common regularization techniques:\n",
    "\n",
    "# L1 Regularization (Lasso): Adds the absolute value of the coefficients as a penalty to the loss function, which can lead to sparse models (some coefficients can become exactly zero).\n",
    "\n",
    "# L2 Regularization (Ridge): Adds the squared value of the coefficients as a penalty, which reduces the magnitude of the coefficients but usually keeps all features.\n",
    "\n",
    "# Elastic Net: Combines L1 and L2 penalties, balancing between feature selection (L1) and coefficient shrinkage (L2)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
