{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping is the process of automatically extracting data from websites. It involves sending requests to web pages, retrieving the HTML content, and then parsing that content to extract specific information.\n",
    "\n",
    "# Why is it Used?: \n",
    "\n",
    "# Data Collection: To gather large amounts of data from the web quickly and efficiently.\n",
    "# Market Research: To monitor competitors, track prices, or analyze trends.\n",
    "# Content Aggregation: To compile data from various sources into a single platform or report.\n",
    "\n",
    "# Three Areas Where Web Scraping is Used:\n",
    "\n",
    "# E-commerce: For price comparison, product reviews, and inventory tracking.\n",
    "# Finance: To collect financial data, stock prices, and market trends.\n",
    "# News Aggregation: To gather articles, headlines, and updates from multiple news sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Scraping: Copying and pasting data manually from web pages.\n",
    "# HTML Parsing: Using libraries to parse HTML documents and extract data (e.g., Beautiful Soup, lxml).\n",
    "# Browser Automation: Using tools like Selenium to simulate user interactions and extract data.\n",
    "# APIs: Accessing data through publicly available APIs provided by websites or services.\n",
    "# Web Scraping Frameworks: Using dedicated frameworks like Scrapy, which offer advanced features for large-scale scraping projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautiful Soup is a Python library used for parsing HTML and XML documents. It provides a simple way to navigate, search, and modify the parse tree.\n",
    "\n",
    "# Why is it Used?\n",
    "\n",
    "# Ease of Use: It allows for easy extraction of data from web pages by providing a simple interface to navigate and search the HTML/XML content.\n",
    "# Handling Complex HTML: It can handle malformed or poorly structured HTML and XML, making it more robust for real-world web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask is a lightweight web framework for Python. It is used in web scraping projects for the following reasons:\n",
    "\n",
    "# Web Interface: To create a web interface where users can input parameters or trigger scraping tasks.\n",
    "# API Endpoints: To provide RESTful API endpoints for interacting with the scraping logic.\n",
    "# Integration: To integrate the scraping process with other web services or user interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon EC2 (Elastic Compute Cloud):\n",
    "\n",
    "# Use: Provides scalable virtual servers for running web scraping scripts and applications. It can be used to deploy and run the web scraping code.\n",
    "# Amazon S3 (Simple Storage Service):\n",
    "\n",
    "# Use: Stores the scraped data in a scalable and durable manner. It can be used to save large datasets or backup the results of the scraping process.\n",
    "# Amazon RDS (Relational Database Service):\n",
    "\n",
    "# Use: Manages relational databases to store and query structured data extracted from the web. It can be used to handle complex queries and data analysis.\n",
    "# AWS Lambda:\n",
    "\n",
    "# Use: Executes code in response to events, such as scheduling regular scraping tasks without needing to manage servers. It is useful for running lightweight scraping scripts on a schedule.\n",
    "# Amazon CloudWatch:\n",
    "\n",
    "# Use: Monitors and logs the performance of the web scraping application and AWS resources. It provides insights into application health and usage metrics.\n",
    "# AWS Glue:\n",
    "\n",
    "# Use: Provides data integration and ETL (Extract, Transform, Load) capabilities to prepare and transform the scraped data before analysis or storage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
